{
    // URL for OpenAI API. Change when using custom, OpenAI compatible API.
    // It must start with http:// or https://, which selects protocol for connection. Use http:// when using localhost.
    // Selected parts of code and prompt will be sent to that URL, so make sure, you have all necessary permission to do so.
    // Example: "http://localhost:11434" (assuming Ollama is running on localhost)
    "url": "https://api.openai.com",

    // Your openAI token
    "token": "",

    // Apply Sublime Text markdown syntax highligh to OpenAI completion output panel text.
    // Affects only `"prompt_mode": "panel"`.
    // `MultimarkdownEditing` package highly recommended to install to apply syntax highlight for a wider range of languages.
    "markdown": true,

    // Minimum amount of characters selected to perform completion.
    "minimum_selection_length": 10,

    // Status bar hint setup that presents major info about currently active assistant setup (from the array of assistnat objects above)
    // Possible options:
    //  - name: User defined assistant setup name
    //  - prompt_mode: Model output prompt mode (panel|append|insert|replace)
    //  - chat_model: Which OpenAI model are used within this setup (e.g. gpt-4, gpt-3.5-turbo-16k).
    //
    // You're capable to mix these whatewer you want and the text in status bar will follow.
    "status_hint": [
        // "name",
        // "prompt_mode",
        // "chat_model"
     ],

    // Proxy setting
    "proxy": {
        // Proxy address
        "address": "",

        // Proxy port
        "port": 8080,

        // Proxy username
        "username": "",

        // Proxy password
        "password": ""
    },

    "assistants": [
        {
            // A string that will presented in command palete.
            "name": "Example", // **REQUIRED**

            // Mode of how plugin should promts its output, available options:
            //  - panel: prompt would be output in output panel, selected text wouldn't be affected in any way.
            //  - append: prompt would be added next to the selected text.
            //  - insert: prompt would be inserted instead of a placeholder within a selected text.
            //  - replace: prompt would overwite selected text.
            //
            // All cases but `panel` required to some text be selected beforehand.
            // The same in all cases but `panel` user type within input panel will be treated by a model
            // as `system` command, e.g. instruction to action.
            "prompt_mode": "panel", // **REQUIRED**

            // The model which will generate the chat completion.
            // Generaly here should be either "gpt-3.5.-turbo|gpt-4|gpt-3.5-turbo-instruct" or their specified versions.
            // If using custom API, refer to their documentation for supported models.
            // Learn more at https://beta.openai.com/docs/models
            "chat_model": "gpt-3.5-turbo", // **REQUIRED**

            // ChatGPT model knows how to role, lol
            // It can act as a different kind of person. Recently in this plugin it was acting
            // like as a code assistant. With this setting you're able to set it up more precisely.
            // E.g. "You are (rust|python|js|whatewer) developer assistant", "You are an english tutor" and so on.
            "assistant_role": "You are a senior code assitant", // **REQUIRED**

            // Placeholder requires for performing `insert` action.
            // It got passed to OpenAI as an additional system command for additional context, to let it know
            // what it sould replace within the user's input.
            //
            // There's no need to dublicate this placeholder within assistant role property
            //
            // NOTE: Works quite unreliable with gpt-3.5-turbo model so far, but OpenAI promissed to fix this.
            // WARNING: This property would be passed to a model if it's provided and it might be a thing that bringing a mess in your dialog and in model answers.
            "placeholder": "[PLACEHOLDER]",

            // What sampling temperature to use, between 0 and 2.
            // Higher values like 0.8 will make the output more random,
            // while lower values like 0.2 will make it more focused and deterministic.
            //
            // OpenAI generally recommend altering this or `top_p` but not both.
            "temperature": 1,

            // The maximum number of tokens to generate in the completion.
            // The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
            // (One token is roughly 4 characters for normal English text)
            // Does not affect editing mode.
            "max_tokens": 2048,

            // An alternative to sampling with temperature, called nucleus sampling,
            // where the model considers the results of the tokens with `top_p` probability mass.
            // So 0.1 means only the tokens comprising the top 10% probability mass are considered.
            //
            // OpenAI generally recommend altering this or `temperature` but not both.
            "top_p": 1,

            // Number between -2.0 and 2.0.
            // Positive values penalize new tokens based on their existing frequency in the text so far,
            // decreasing the model's likelihood to repeat the same line verbatim.
            // docs: https://platform.openai.com/docs/api-reference/parameter-details
            "frequency_penalty": 0,

            // Number between -2.0 and 2.0.
            /// Positive values penalize new tokens based on whether they appear in the text so far,
            // increasing the model's likelihood to talk about new topics.
            // docs: https://platform.openai.com/docs/api-reference/parameter-details
            "presence_penalty": 0,
        },
        // Instructions //
        {
            "name": "Insert instruction example",
            "prompt_mode": "insert",
            "chat_model": "gpt-4", // works unreliable with gpt-3.5-turbo yet.
            "assistant_role": "Insert code or whatever user will request with the following command instead of placeholder with respect to senior knowlage of in Python 3.8 and Sublime Text 4 plugin API",
            "max_tokens": 4000,
            "placeholder": "## placeholder" // it's a good fit for a placeholder to be a comment.
        },
        {
            "name": "Append instruction example",
            "prompt_mode": "append",
            "chat_model": "gpt-4", // works unreliable with gpt-3.5-turbo now.
            "assistant_role": "Insert code or whatever user will request with the following command instead of placeholder with respect to senior knowlage of in Python 3.8 and Sublime Text 4 plugin API",
            "max_tokens": 4000,
        },
        {
            "name": "Replace instruction example",
            "prompt_mode": "replace",
            "chat_model": "gpt-4", // works unreliable with gpt-3.5-turbo now.
            "assistant_role": "Apply the change requested by the user to the code with respect to senior knowlage of in Python 3.8 and Sublime Text 4 plugin API",
            "max_tokens": 4000,
        },

        // Other useful ideas //
        {
            "name": "ST4 Plugin", // It's unecessary to mention a model here, because there's separate filed for just this in status bar hint setting.
            "prompt_mode": "panel",
            "chat_model": "gpt-4",
            "assistant_role": "You are senior Sublime Text 4 editor plugin development and Python code assitant",
            "max_tokens": 4000,
        },
        {
            "name": "ST4 Plugin", // You can share the same `assistant_role` through different models.
            "prompt_mode": "panel",
            "chat_model": "gpt-3.5-turbo-16k",
            "assistant_role": "You are senior Sublime Text 4 editor plugin development and Python code assitant",
            "max_tokens": 4000,
        },
        {
            "name": "UIKit & Combine",
            "prompt_mode": "panel",
            "chat_model": "gpt-4-0613",
            "assistant_role": "You are senior UIKit and Combine code assitant",
            "max_tokens": 4000,
        },
        {
            "name": "Social Researcher",
            "prompt_mode": "panel",
            "chat_model": "gpt-4-0613",
            "assistant_role": "You are senior social researcher",
            "max_tokens": 4000,
        },
        {
            "name": "Phd in Computer Science",
            "prompt_mode": "panel",
            "chat_model": "gpt-4-0613",
            "assistant_role": "You are Phd in computer science, ML scope",
            "max_tokens": 4000,
        },
        {
            "name": "Corrector",
            "prompt_mode": "replace",
            "chat_model": "gpt-4-0613",
            "assistant_role": "Fix provided text with the correct and sounds English one, you are strictly forced to skip any changes in such its part that have not rules violation within them",
            "max_tokens": 4000,
        },
        {
            "name": "English Tutor",
            "prompt_mode": "panel",
            "chat_model": "gpt-4-0613",
            "assistant_role": "You are senior English tutor: (1)you fix the input that user provides, (2) you emphasize the fixed words with md bold, and format the whole edited text with md quote, (3) right after that you provide a **detailed explanation** of what English rules were violated and **why**. (4) If user inputs to you an clarifation message like that he/she meant a bit different in original sentense as you've provided â€” you counts it as a command, not as a new sentence to format and provide fixed version with incremental updates in explanation. (5) In all other context among mentioned above you have to be as low wordy as possibly can.",
            "max_tokens": 4000,
        },
        {
            "name": "bash/zsh converter",
            "prompt_mode": "panel",
            "chat_model": "gpt-4-0613",
            "assistant_role": "You are bash/zsh man pages into bash/zsh completion plugins converter. You are strictly restricted to prompting me anything but the script itself, not a single word along it. Within the requested scrip you should to provide only the `_arguments`",
            "max_tokens": 4000,
        },
        {
            "name": "Bash & Git assitant",
            "prompt_mode": "panel",
            "chat_model": "gpt-4-0613",
            "assistant_role": "You are bash and git senior assitant",
            "max_tokens": 4000,
        },
        {
            "name": "CoreImage & AVFoundation assitant",
            "prompt_mode": "panel",
            "chat_model": "gpt-4-0613",
            "assistant_role": "You are senior of CoreImage, Vision, CoreML and AVFoundation code assitant, all your answer you're providing after taking a deep breath and going step by step.",
            "max_tokens": 4000,
        },
        {
            "name": "Pytorch assitant",
            "prompt_mode": "panel",
            "chat_model": "gpt-4-0613",
            "assistant_role": "You are senior Pytorch and LLM/SD code assitant",
            "max_tokens": 4000,
        },
    ]
}
