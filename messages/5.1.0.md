# 5.1.0

1. Function working stable (at least with OpenAI models). You're toggling on `tools": true` in assistant settings and it provides for llm set of tools to observe project and to apply partial changes (the way codex/aider does). Despite that 5.0.0 already has implementation of tools, patching have been worked really bad, current implementation works quite stable, in terms of o4-mini low is capable to apply patch even (or especially) if it takes few attempts for it to make it right.
2. Phantom mode can now be toggled to be one shot. I.e. if you prefer (as I do) to have a conversation within the tab but wish to get a clarification of a certain points about the chat or whatever that are too off-topic to be placed in permanent, set `"phantom_permanent": false,` within `chat_presentation` dict.

## tldr;

I got bored and rewrote the whole thing in rust completely. There's not that much brand new features so far, but a lot of them are coming, since the core of the package is now much more reliable and less tangled. [Here it is btw](https://github.com/yaroslavyaroslav/llm_runner).

## Features

1. The core of the plugin is implemented in rust, thus it has become a way faster and reliable.
2. Context passing enhancement:
    - files/sheets passes as references now, i.e. the model always sees the actual version of the file with the changes applied.
    - they're togglable now, i.e. you pick those that you want to include, call a command on it and then all the time along they're passed to the server until you toggle them back off.
    - built in output panels content passing, e.g. build systems and lsp diagnostic outputs can be passed with a command.
3. Model picker command now supports nested list flow, thus you can switch between view modes and the models on the fly.
4. `AssistantSettings` now provides `"api_type"`, where the options are `"plain_text"`[default], `"open_ai"` and `"antropic"`[not implemented]. This is the ground work already done to provide Claude and all the rest of the custom services support in the nearest future. Please take a look at the assistant settings part if you're curious about the details.
5. Chat history and picked model now can be stored in arbitrary folder (see readme for details).
6. Functions support, there are few built in functions provided to allow model to manage the code:
    - `apply_patch` to replace an exact text snippet with the new one, 
    - `replace_text_for_whole_file` to replace the whole file content, 
    - `read_region_content` lets a model to read the file's content, 
    - `get_working_directory_content` lets a model to obtain the project structure.
7. Phantom has new action `Add to History`, which is does what you're expecting it would â€” adds question/answer to the history if you want to continue the chat on top of it.
8. Phantom's `insert`/`replace`/`append` commands might be switched to work with the code blocks only with `"chat_presentation": {"phantom_integrate_code_only": true}` setting. Thanks to @dam024
9. Thinking models are supported within the phantoms as well, so the thinking toggle is added in its actions available. Thanks to @dam024 

## Breaking changes

- `prompt_mode`: ignored and has to be deleted, use quick panel instead
- general `token` property ignored, set token for each of assistant which requires it
- `insert`/`replace`/`append` buffer commands deleted.

## Roadmap

1. OpenAI Responses API migration (Say hello to built in web search support).
1. Claude/deepseek/gemini support (i.e. strict messages order).
2. View mode goodies implementation, better chat structure, code blocks quick actions, history management.
3. Input panel to output panel for request replacement.

Ps: As the most considerate of you might have noticed, MCP support is missed from here. That's because I myself am all about to focus on OpenAI models and APIs support, bc I have no effort to maintain models that I'm not using by myself. The most of the foundation for Anthropic models support is already made, so if someone has it you're very welcome to accomplish the task [here](https://github.com/yaroslavyaroslav/llm_runner/issues/1).
